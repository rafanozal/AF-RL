{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOOhUERW8yQfwVnmyBNWVJV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","from scipy.special import expit as logistic_sigmoid\n","from scipy.optimize import minimize\n","from sklearn.metrics import roc_curve, auc as sklearn_auc\n","from scipy.stats import chi2"],"metadata":{"id":"IxWighf7gIvW","executionInfo":{"status":"ok","timestamp":1702359918125,"user_tz":-60,"elapsed":1264,"user":{"displayName":"Rafael Nozal Cañadas","userId":"17018502874477987347"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mx9Tg-RIcDUP"},"outputs":[],"source":["def iso_fe(mdat):\n","    # Convert input to a NumPy array\n","    mdat = np.array(mdat)\n","    n, p_plus_one = mdat.shape\n","    p = p_plus_one - 1\n","\n","    # Call sparsebasis function (assumed to be defined elsewhere)\n","    ans = sparsebasis(mdat, p)\n","    amat0 = ans['amat']\n","    rank = ans['rank']\n","    m = len(amat0) // 2\n","    y = ans['y']\n","    xmat = ans['xmat']\n","\n","    # Construct amat matrix\n","    amat = np.zeros((m, n))\n","    for i in range(m):\n","        amat[i, int(amat0[i, 0])] = -1\n","        amat[i, int(amat0[i, 1])] = 1\n","\n","    # Call coneproj function (assumed to be defined elsewhere)\n","    ans = coneproj(y, amat)\n","    theta = ans['theta']\n","    df = ans['df']\n","\n","    # Return results in a dictionary\n","    return {\n","        'theta': theta,\n","        'df': df,\n","        'xmat': xmat,\n","        'y': y\n","    }"]},{"cell_type":"code","source":["# XW.matrix\n","# Esta función crea una matriz X para todos los puntos de evaluación x0 y\n","# una matriz de ponderaciones W. Aplica una función kernel para calcular\n","# las ponderaciones en función de la distancia entre la matriz x.mat y\n","# cada punto de evaluación x0.mat.\n","\n","def XW_matrix(x_mat, x0_mat, h, K):\n","\n","    p = x_mat.shape[1]\n","    if len(x0_mat.shape) == 1:\n","        x0_mat = x0_mat.reshape(1, p)\n","\n","    ngrid = x0_mat.shape[0]\n","    n = x_mat.shape[0]\n","\n","    print(x_mat)\n","    print(x0_mat)\n","    print(ngrid)\n","    print(n)\n","\n","    #X_mat = [x_mat - np.tile(x0_mat[i, :], (n, 1)) for i in range(ngrid)]\n","    X_mat = [x_mat - x0_mat.iloc[0] for i in range(ngrid)]\n","\n","    print(X_mat)\n","    print(h)\n","\n","    W_mat = 0\n","\n","    if K == 'epa':\n","        W_mat = [kepa(np.sqrt(np.sum(X_mat[i]**2, axis=1)) / h) / h for i in range(ngrid)]\n","    elif K == 'gauss':\n","        #W_mat = [kgauss(np.sqrt(np.sum(X_mat[i]**2, axis=1)) / h) / h for i in range(ngrid)]\n","\n","\n","        # Compute the Gaussian kernel weights for each row in the DataFrame\n","        W_mat = [kgauss(np.sqrt((X_mat[0]**2).sum(axis=1)) / h_val) / h_val for h_val in h]\n","\n","    return {'X_mat': X_mat, 'W_mat': W_mat}\n","\n","# Local.lik\n","# Esta función calcula la probabilidad logarítmica local para la regresión\n","# logística. Toma un vector de coeficientes beta, una matriz X0.mat\n","# (que es x.mat - x0 para un punto de evaluación específico x0),\n","# la variable de respuesta y y el vector correspondiente de pesos w0.\n","\n","def Local_lik(beta, X0_mat, y, w0):\n","    n, p = X0_mat.shape\n","    X0_mat = np.hstack((np.ones((n, 1)), X0_mat))\n","    u = X0_mat @ beta\n","    p0 = logistic_sigmoid(u)\n","\n","    return -np.sum(2 * w0 * np.where(y, np.log(p0), np.log(1 - p0)))\n","\n","# Local.grad\n","# Esta función calcula el gradiente de la función de verosimilitud\n","# logarítmica local. Utiliza las mismas entradas que Local.lik.\n","\n","def Local_grad(beta, X0_mat, y, w0):\n","    n, p = X0_mat.shape\n","    X0_mat = np.hstack((np.ones((n, 1)), X0_mat))\n","    u = X0_mat @ beta\n","    p0 = logistic_sigmoid(u)\n","\n","    return -2 * (X0_mat.T @ (np.diag(w0) @ (y - p0)))"],"metadata":{"id":"po34zgPtgG2v","executionInfo":{"status":"ok","timestamp":1702362760788,"user_tz":-60,"elapsed":378,"user":{"displayName":"Rafael Nozal Cañadas","userId":"17018502874477987347"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Local.logit\n","#\n","# Esta función se ajusta a un modelo de regresión logística local en cada punto\n","# de evaluación x0 en x0.mat utilizando un ancho de banda especificado h y un\n","# kernel K.\n","#\n","#Devuelve los coeficientes estimados para cada punto de evaluación.\n","def Local_logit(x_mat, y, x0_mat, h, K):\n","    toeval = XW_matrix(x_mat, x0_mat, h, K)\n","    X_matrix = toeval['X_mat']  # List with all design matrices for all x0 estimation points\n","    W_mat = toeval['W_mat']  # Matrix with all weights vectors, for all x0 estimation points\n","    n_grid = x0_mat.shape[0]\n","    p = x_mat.shape[1]\n","\n","    start = np.zeros(p + 1)\n","    betas = np.empty((0, p + 1))\n","\n","    for i in range(n_grid):\n","        w0 = W_mat[i]\n","        X0_mat = X_matrix[i]\n","\n","        f = lambda beta: Local_lik(beta, X0_mat=X0_mat, y=y, w0=w0)\n","        grad = lambda beta: Local_grad(beta, X0_mat=X0_mat, y=y, w0=w0)\n","\n","        mle1 = minimize(f, start, jac=grad, method='BFGS')\n","\n","        betas = np.vstack((betas, mle1.x))\n","\n","    return betas[1:]  # Exclude the initial row of NA values\n","\n","# h.CV\n","# Esta función realiza una validación cruzada para seleccionar el ancho de\n","# banda óptimo de una cuadrícula de anchos de banda candidatos h.grid.\n","#\n","# Devuelve el ancho de banda que minimiza la puntuación de validación cruzada.\n","def h_CV(h_grid, x_mat, y, K='gauss'):\n","    n_h = len(h_grid)\n","    n = x_mat.shape[0]\n","\n","    def cv_score(h):\n","        betas_i = np.array([Local_logit(x_mat=np.delete(x_mat, i, axis=0), y=np.delete(y, i), x0_mat=x_mat[i, :], h=h, K=K) for i in range(n)])\n","        beta0_i = betas_i[:, 0]\n","        cv = np.sum((y - logistic_sigmoid(beta0_i))**2) / n\n","        return cv\n","\n","    cv_values = np.array([cv_score(h) for h in h_grid])\n","    ind_cv = np.argmin(cv_values)\n","    hcv = h_grid[ind_cv]\n","    return hcv"],"metadata":{"id":"6KcRWPLthMzt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# logit.lik\n","# Esta función calcula la probabilidad logarítmica para un modelo de regresión\n","# logística dados los coeficientes beta, la matriz de entrada x, y la variable\n","# de respuesta y.\n","def logit_lik(beta, x, y):\n","    if x.ndim == 1:\n","        x = np.hstack(([1], x))\n","        u = np.dot(x, beta)\n","    else:\n","        x = np.column_stack((np.ones(x.shape[0]), x))\n","        u = x @ beta\n","    p = logistic_sigmoid(u)\n","    return -np.sum(np.where(y, np.log(p), np.log(1 - p)))\n","\n","# logit.grad\n","# Esta función calcula el gradiente de la función log-verosimilitud para un\n","# modelo de regresión logística.\n","def logit_grad(beta, x, y):\n","    if x.ndim == 1:\n","        x = np.hstack(([1], x))\n","        u = np.dot(x, beta)\n","    else:\n","        x = np.column_stack((np.ones(x.shape[0]), x))\n","        u = x @ beta\n","    p = logistic_sigmoid(u)\n","    return -2 * (x.T @ (y - p))\n","\n","# mon.lik\n","# Esta función se ajusta a un modelo de regresión logística que\n","# utiliza la optimización restringida (para garantizar la monotonicidad de\n","# los coeficientes) y devuelve los coeficientes estimados y las probabilidades.\n","def mon_lik(mdat):\n","    n, p_with_y = mdat.shape\n","    p = p_with_y - 1  # p = number of components of the system\n","    x = mdat[:, :p]\n","    y = mdat[:, p]\n","\n","    f = lambda beta: logit_lik(beta, x=x, y=y)\n","    grad = lambda beta: logit_grad(beta, x=x, y=y)\n","\n","    Amat = np.eye(p + 1)\n","    Amat[0, 0] = -1\n","    bvec = np.zeros(p + 2)\n","    start = np.array([-0.1] + [0.1] * p)\n","\n","    # Adding an additional constraint for monotonicity\n","    Amat = np.column_stack((Amat, np.ones(p + 1)))\n","    bvec = np.append(bvec, 0)\n","\n","    constraints = {'type': 'eq', 'fun': lambda beta: Amat.T @ beta - bvec}\n","\n","    mle1 = minimize(f, start, jac=grad, method='SLSQP', constraints=constraints)\n","    g_beta = mle1.x\n","\n","    # Predicted probability\n","    if x.ndim == 1:\n","        x = np.hstack(([1], x))\n","        u = np.dot(x, g_beta)\n","    else:\n","        x = np.column_stack((np.ones(x.shape[0]), x))\n","        u = x @ g_beta\n","    prob = logistic_sigmoid(u)\n","\n","    return {'g_beta': g_beta, 'prob': prob}"],"metadata":{"id":"ONPTJTPGiTl2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# y.pred\n","# Esta función predice resultados binarios en función de la probabilidad y\n","# un umbral alfa dado.\n","def y_pred(prob, alpha):\n","    y_0 = np.where(prob < alpha, 0, 1)\n","    return y_0\n","\n","# roc.curve: Esta función calcula la curva ROC variando el umbral y calculando\n","# la sensibilidad y la especificidad.\n","def roc_curve_custom(y, prob):\n","    nc = 100\n","    thresholds = np.linspace(0.05, 0.95, nc)\n","    S_c = np.zeros(nc)\n","    E_c = np.zeros(nc)\n","    y_mat = np.empty((len(y), nc))\n","\n","    for i, c in enumerate(thresholds):\n","        y_mat[:, i] = np.where(prob < c, 0, 1)\n","        y_c = y_mat[:, i]\n","\n","        VN = np.sum((y_c == 0) & (y == 0))  # True Negative\n","        FN = np.sum((y_c == 0) & (y == 1))  # False Negative\n","        FP = np.sum((y_c == 1) & (y == 0))  # False Positive\n","        VP = np.sum((y_c == 1) & (y == 1))  # True Positive\n","        S_c[i] = VP / (FN + VP)  # Sensitivity\n","        E_c[i] = VN / (FP + VN)  # Specificity\n","\n","    i_optim = np.argmin((1 - S_c)**2 + (1 - E_c)**2)\n","    c0 = thresholds[i_optim]\n","    y_c = y_mat[:, i_optim]\n","    return {'c0': c0, 'y_c': y_c}\n","\n","# auc\n","# Esta función calcula el área bajo la curva ROC (AUC) comparando las\n","# probabilidades de verdaderos positivos y verdaderos negativos.\n","def auc_custom(y, prob):\n","    pi0 = prob[y == 0]\n","    pi1 = prob[y == 1]\n","    roc_mat = np.array([1 * (pi0_i < pi1) for pi0_i in pi0])\n","    return roc_mat.sum() / (len(pi0) * len(pi1))\n","\n","# hoslem.gof\n","# Esta función realiza la prueba de bondad de ajuste de Hosmer-Lemeshow para\n","# modelos de regresión logística\n","def hoslem_gof(y, prob):\n","    k = len(y) // 10\n","    indices = np.argsort(prob)\n","    Ei_1 = [np.sum(prob[indices[(i * k):((i + 1) * k)]]) for i in range(10)]\n","    Oi_1 = [np.sum(y[indices[(i * k):((i + 1) * k)]]) for i in range(10)]\n","    Ei_0 = [10 - e for e in Ei_1]\n","    Oi_0 = [10 - o for o in Oi_1]\n","    HL = np.sum((np.array(Oi_1) - np.array(Ei_1))**2 / np.array(Ei_1)) + np.sum((np.array(Oi_0) - np.array(Ei_0))**2 / np.array(Ei_0))\n","    return chi2.sf(HL, df=8)"],"metadata":{"id":"UKxVSeM2jqhW"},"execution_count":null,"outputs":[]}]}